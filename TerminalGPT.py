'''
Script for using OpenAI's GPT-models on the terminal.

-Mikko Lempinen
'''
import ast
import traceback
import openai

def open_file(filepath):
    '''
    Open a file.

    :param filepath: (string) Path to the file.
    :return: Opened file.
    '''
    with open(filepath, 'r', encoding='utf-8') as infile:
        return infile.read()
    
def gpt35t_completion(prompt, model='gpt-4', temp=0.4, top_p=1.0, max_tokens=1000, freq_pen=0.7, pres_pen=0.0, stop=['"role": "assistant", "content": " "', 'user:', 'User:', 'USER:']):
    '''
    Get a completion from OpenAI's GPT LLM to a prompt.

    :param prompt: (list) List of dictionaries where each dictionary is in format: {'role': 'user', 'content': 'Hello!'}
    :(Optional) param model: (string) GPT model to use. Default: gpt-3.5-turbo
    :(Optional) param temp: (int) See OpenAI documentation. Default: 0.4
    :(Optional) param top_p: (int) See OpenAI documentation. Default: 1.0
    :(Optional) param max_tokens: (int) See OpenAI documentation. Default: 1000
    :(Optional) param freq_pen: (int) See OpenAI documentation. Default: 0.7
    :(Optional) param pres_pen: (int) See OpenAI documentation. Default: 0.0
    :(Optional) param stop: (list) Stop signals for the GPT model (MAX 4).

    :return text: (string) Completion generated by the GPT model.
    :return tokens_total: (int) Total amount of tokens used on the prompt + completion.
    '''
    text = ''
    tokens_total = 0
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=prompt,
            temperature=temp,
            max_tokens=max_tokens,
            top_p=top_p,
            frequency_penalty=freq_pen,
            presence_penalty=pres_pen,
            stop=stop)
    except openai.error.InvalidRequestError:
        # Most likely token limit exceeded.
        if len(prompt) < 3:
            # TODO: Cut some logs off if they exceed 4000 tokens.
            return text, tokens_total
        for item in range(int(len(prompt)/2)):
            prompt.pop(2)
        text, tokens_total = gpt35t_completion(prompt)
        return text, tokens_total
    except Exception:
        traceback.print_exc()
        return "Exception occured", 0
    text = response['choices'][0]['message']['content'].strip()
    tokens_total = response['usage']['total_tokens']
    return text, tokens_total

if __name__ == '__main__':
    # OpenAI API key
    openai.api_key = open_file('openaiapikey.txt')
    tokens = 0
    # Type your system message here to guide the GPT model's behaviour
    conversation = [{"role": "system", "content" : "You are a university research assistant. Your main objective is to help the user write as sophisticated text for research papers as possible."}]
    while True:
        # If token amount in the conversation starts to near max, remove older half of the conversation from the AI's memory.
        if tokens > 3000:
            for i in range(int(len(conversation)/2)):
                conversation.pop(1)
        user_input = input('USER: ')
        # Append the user prompt into conversation.
        conversation.append('''{''' + f''''role': 'user', 'content': '{user_input.replace("'", '"')}' ''' + '}')
        conversation[-1] = ast.literal_eval(conversation[-1].replace('\r','\\r').replace('\n','\\n'))
        # Append a stop signal for AI to the end of convo.
        conversation.append('{"role": "assistant", "content": " "}')
        conversation[-1] = ast.literal_eval(conversation[-1].replace('\r','\\r').replace('\n','\\n'))
        # Get response from AI
        #print(f"\n\nConverstaion: {conversation}\n\n")
        response, tokens = gpt35t_completion(conversation)
        #print(f"Response: {response}\nTokens: {tokens}")
        if response == "Exception occured":
            print("Something went wrong. Please try again later.")
            exit()
        print('Pinocchio:', response)
        # Append the AI's response into convo.
        conversation[-1] = '''{''' + f''''role': 'assistant', 'content': '{response.replace("'", '"')}' ''' + '}'
        conversation[-1] = ast.literal_eval(conversation[-1].replace('\r','\\r').replace('\n','\\n'))